{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c0a52-e434-4202-b9a4-f725790792f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir raw_data\n",
    "!mkdir raw_data/gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdaace-7b52-4d9a-b008-dde6e3dd65c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python3 -m venv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135a4cc1-9345-4743-bf52-7e5afb5c2c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fde184e-f27b-49e6-9bd3-7febe4e6c27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install datasets==1.4.0 py7zr wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97915ea5-230b-44f4-8ace-1c32ba67a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057a7f4-481d-4b05-bdd6-50aa7526eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658f71e-38fe-4a7a-b45c-476fecca6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./entail2/dataloader/gym2entail_multitask.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3955834d-911e-49c2-9e1b-150f18c97ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "python ./entail2/runner/runner.py \\\n",
    "--learning_rate 1e-5 \\\n",
    "--warmup_ratio 0.06 \\\n",
    "--train_batch_size 32 \\\n",
    "--num_train_epochs 10 \\\n",
    "--bert_name bert \\\n",
    "--model_name efl_no_cl \\\n",
    "--use_sampler \\\n",
    "--mode train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30184d7f-c293-43c4-be0e-ad6e4f4cac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./scripts/gen_singletask_test.py \\\n",
    "--data_dir raw_data/gym \\\n",
    "--task_dir custom_dataset_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef77fe-d8e8-4b19-a2a4-856ca63478dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/gen_singletask_zeroshot_support.py --data_dir raw_data/gym --task_dir custom_dataset_4 --shots 1 --times 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60f7e52-6868-4622-8902-1f389a27d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python entail2/runner/runner.py \\\n",
    "--data_dir raw_data/gym \\\n",
    "--task_dir custom_dataset_4  \\\n",
    "--model entail2      \\\n",
    "--test_times 1 \\\n",
    "--test_shots 10\\\n",
    "--mode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eea894-8fa2-4025-8050-562e63355e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python entail2/runner/runner.py --learning_rate 1e-5 --warmup_ratio 0.06 --train_batch_size 32 --num_train_epochs 10 --bert_name bert --model_name unifew --use_sampler --mode train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c3390-6ef3-4c57-aade-7b36fb678988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from googletrans import Translator\n",
    "import random\n",
    "\n",
    "# translate sentences using Google Translate API\n",
    "def translate_sentences(sentences, target_languages):\n",
    "    translations = {}\n",
    "    translator = Translator()\n",
    "\n",
    "    for lang in target_languages:\n",
    "        try:\n",
    "            translations[lang] = [translator.translate(sentence, dest=lang).text for sentence in tqdm(sentences, desc=f'Translating to {lang}')]\n",
    "        except Exception as e:\n",
    "            print(f\"Translation to {lang} failed. Error: {e}\")\n",
    "            translations[lang] = []  \n",
    "\n",
    "    return translations\n",
    "\n",
    "# process a folder and translate a percentage of the dataset\n",
    "def process_folder(folder_path, target_languages, translation_percentage=0.7):\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        # Check if the file is a training or testing file based on its name\n",
    "        if 'train' in file or 'test' in file:\n",
    "            # Load the dataset\n",
    "            print(\"File\", file, \" Started\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "                # Determine the number of sentences to translate based on the percentage\n",
    "                num_sentences_to_translate = int(len(df) * translation_percentage)\n",
    "\n",
    "                # Randomly select sentences to translate\n",
    "                sentences_to_translate = random.sample(df.iloc[:, 0].tolist(), num_sentences_to_translate)\n",
    "\n",
    "                # Translate sentences\n",
    "                translations = translate_sentences(sentences_to_translate, target_languages)\n",
    "\n",
    "                # Distribute the translations equally across the languages\n",
    "                num_languages = len(target_languages)\n",
    "                num_sentences_per_language = num_sentences_to_translate // num_languages\n",
    "\n",
    "                # Create a mapping of original sentences to translated sentences for each language\n",
    "                translation_mapping = {}\n",
    "                for i, lang in enumerate(target_languages):\n",
    "                    start_idx = i * num_sentences_per_language\n",
    "                    end_idx = start_idx + num_sentences_per_language\n",
    "                    translation_mapping.update(zip(sentences_to_translate[start_idx:end_idx], translations[lang]))\n",
    "\n",
    "                # Apply translations to the DataFrame\n",
    "                df['translated_sentence'] = df.iloc[:, 0].map(translation_mapping).fillna(df.iloc[:, 0])\n",
    "\n",
    "                # Save the updated DataFrame to the same file in TSV format\n",
    "                df.to_csv(file_path, sep='\\t', index=False)\n",
    "                print(\"File\", file, \" completed\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset folder\n",
    "    dataset_folder = './raw_data/gym'\n",
    "\n",
    "    # Set the target languages for translation\n",
    "    target_languages = ['fr', 'es', 'de', 'ja', 'zh']\n",
    "\n",
    "    # Set the translation percentage\n",
    "    translation_percentage = 0.7\n",
    "\n",
    "    # Get a list of all folders in the dataset folder\n",
    "    folders = [f for f in os.listdir(dataset_folder) if os.path.isdir(os.path.join(dataset_folder, f))]\n",
    "\n",
    "    # Process each folder\n",
    "    for folder in folders:\n",
    "        print(\"Folder\", folder, \" Started\")\n",
    "        folder_path = os.path.join(dataset_folder, folder)\n",
    "        process_folder(folder_path, target_languages, translation_percentage)\n",
    "        print(\"Folder\", folder, \" completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10dd5e1-c1ee-4057-920a-b31b01e08271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# current working directory\n",
    "data_directory = './swear.json'\n",
    "\n",
    "# List all files in the specified directory\n",
    "for dirname, _, filenames in os.walk(data_directory):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cd6690-f598-4e1b-8f51-ac0c932f8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# CSV file name\n",
    "csv_file = 'spam_data_hi.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame \n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# output TSV file name\n",
    "tsv_file = 'spam_data_hi.tsv'\n",
    "\n",
    "# TSV file with tab separator\n",
    "df.to_csv(tsv_file, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920ef60-8583-466b-8fed-41c92120a181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# path to your folder containing TSV files\n",
    "folder_path = './raw_data/gym/custom_dataset'\n",
    "\n",
    "tsv_files = [file for file in os.listdir(folder_path) if file.endswith('.tsv')]\n",
    "\n",
    "# Loop through each TSV file\n",
    "for file in tsv_files:\n",
    "    # Read the TSV file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(folder_path, file), sep='\\t')\n",
    "\n",
    "    # Split the data into train and test sets (80% train, 20% test)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define new file names for train and test datasets\n",
    "    train_file_name = file.replace('.tsv', '_train.tsv')\n",
    "    test_file_name = file.replace('.tsv', '_test.tsv')\n",
    "\n",
    "    # Save the train and test datasets to new files\n",
    "    train_df.to_csv(os.path.join(folder_path, train_file_name), sep='\\t', index=False)\n",
    "    test_df.to_csv(os.path.join(folder_path, test_file_name), sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f15884-55bb-4b7b-98e7-e66b88ee9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import os\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import regex\n",
    "\n",
    "def remove_emojis(text):\n",
    "    \"\"\"\n",
    "    Remove emojis from text.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        emoji_pattern = regex.compile(\"[\"\n",
    "                                    u\"\\U0001F600-\\U0001F64F\"\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"\n",
    "                                    u\"\\U0001F700-\\U0001F77F\"\n",
    "                                    u\"\\U0001F780-\\U0001F7FF\"\n",
    "                                    u\"\\U0001F800-\\U0001F8FF\"\n",
    "                                    u\"\\U0001F900-\\U0001F9FF\"\n",
    "                                    u\"\\U0001FA00-\\U0001FA6F\"\n",
    "                                    u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                                    u\"\\U00002702-\\U000027B0\"\n",
    "                                    \"]+\", flags=regex.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Process all TSV files in a folder that contain \"train\" in their names.\n",
    "    \"\"\"\n",
    "    tsv_files = [file for file in os.listdir(folder_path) if file.endswith('.tsv') and 'train' in file]\n",
    "\n",
    "    for file in tsv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        try:\n",
    "            # Read the TSV file into a DataFrame\n",
    "            with open(file_path, 'r', errors='replace') as file_content:\n",
    "                df = pd.read_csv(file_content, sep='\\t')\n",
    "\n",
    "            # Remove emojis from the first column\n",
    "            df.iloc[:, 0] = df.iloc[:, 0].apply(remove_emojis)\n",
    "\n",
    "            # Save the DataFrame back to the TSV file\n",
    "            df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error processing file: {file_path}\")\n",
    "            print(f\"Error details: {e}\")\n",
    "\n",
    "# main folder containing subfolders\n",
    "main_folder = './raw_data/gym'\n",
    "\n",
    "# Process files in all subfolders\n",
    "for subfolder in os.listdir(main_folder):\n",
    "    subfolder_path = os.path.join(main_folder, subfolder)\n",
    "\n",
    "    # Check if the subfolder is a directory\n",
    "    if os.path.isdir(subfolder_path):\n",
    "        process_files_in_folder(subfolder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c229e9cb-9a49-409c-a3f5-d1886779d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# path to the zip file\n",
    "zip_file_path = './raw_data/gym/custom_dataset/Test1.zip'\n",
    "\n",
    "# extraction directory\n",
    "extract_to = './raw_data/gym/custom_dataset'\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    \n",
    "    # Extract all contents into the specified directory\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(f'Contents of {zip_file_path} have been extracted to {extract_to}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94177c0d-f48b-4d77-b946-937cdd665807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_tsv(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all files in the input folder\n",
    "    files = os.listdir(input_folder)\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Form the input and output file paths\n",
    "            input_path = os.path.join(input_folder, file)\n",
    "            output_path = os.path.join(output_folder, file.replace(\".csv\", \".tsv\"))\n",
    "\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(input_path)\n",
    "\n",
    "            # Save the DataFrame to a TSV file\n",
    "            df.to_csv(output_path, sep='\\t', index=False)\n",
    "\n",
    "            print(f\"Converted {file} to {output_path}\")\n",
    "\n",
    "# input and output folders\n",
    "input_folder = './raw_data/gym/custom_dataset'\n",
    "output_folder = './raw_data/gym/custom_dataset'\n",
    "\n",
    "# function call to convert CSV to TSV\n",
    "convert_csv_to_tsv(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09923164-461c-4700-b0f6-0b3f80b6af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = './raw_data/gym/custom_dataset_13/custom_dataset_13_test.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "df.to_csv(file_path, sep='\\t', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24cc822-b1b9-4679-ab72-91430ba32530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
